{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nIoLgk5FNPec"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9miEb-FZ1GUW"
      },
      "outputs": [],
      "source": [
        "pip install tensorflow\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h_OdIicFNS20"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Conv1D, MaxPooling1D, GlobalMaxPooling1D, Dropout\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "import os\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nVtoWnvrNXaT"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv(r\"path to data\", encoding='latin', names=['polarity', 'id', 'text','lang'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f_p6JJ1tNhCE"
      },
      "outputs": [],
      "source": [
        "# Processing label of training/testing data\n",
        "label_data = data['polarity'].values\n",
        "senti = [0, 4]\n",
        "mapping = {}\n",
        "for x in range(len(senti)):\n",
        "    mapping[senti[x]] = x\n",
        "\n",
        "# Integer representation\n",
        "for x in range(len(label_data)):\n",
        "    label_data[x] = mapping[label_data[x]]\n",
        "\n",
        "# Convert to one-hot encoding\n",
        "y_data = to_categorical(label_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FklPrn6rNmu0"
      },
      "outputs": [],
      "source": [
        "# Text cleaning function\n",
        "def text_cleaner(line):\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                               u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
        "                               u\"\\U00002702-\\U000027B0\"\n",
        "                               u\"\\U00002702-\\U000027B0\"\n",
        "                               u\"\\U000024C2-\\U0001F251\"\n",
        "                               u\"\\U0001f926-\\U0001f937\"\n",
        "                               u\"\\U00010000-\\U0010ffff\"\n",
        "                               u\"\\u2640-\\u2642\"\n",
        "                               u\"\\u2600-\\u2B55\"\n",
        "                               u\"\\u200d\"\n",
        "                               u\"\\u23cf\"\n",
        "                               u\"\\u2066\"\n",
        "                               u\"\\u2069\"\n",
        "                               u\"\\u23e9\"\n",
        "                               u\"\\u231a\"\n",
        "                               u\"\\ufe0f\"  # dingbats\n",
        "                               u\"\\u3030\"\n",
        "                               \"]+\", flags=re.UNICODE)\n",
        "    punc = '''؛₹|!‼→¿()-[]{};–৷।:،۔•„'“‘—’”\"…\\,<>=./?@#$%^&*_~»«'''\n",
        "    line = re.sub(r'http\\S+', '', line)\n",
        "    line = emoji_pattern.sub(r'', line)\n",
        "    line = bytes(line, 'utf-8').decode('utf-8', 'ignore')\n",
        "    for ele in line:\n",
        "        if ele in punc:\n",
        "            line = line.replace(ele, \"\")\n",
        "            line = re.sub(r\"^\\s+|\\s+$\", \"\", line)\n",
        "            line = re.sub(' +', ' ', line)\n",
        "    return line"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CRDT1r6UPEGn"
      },
      "outputs": [],
      "source": [
        "# Tokenize and convert the processed documents to sequences\n",
        "data_x = data['text'].tolist()\n",
        "raw_docs_train, raw_docs_test, y_train, y_test = train_test_split(data_x, y_data, test_size=0.2, random_state=4)\n",
        "\n",
        "MAX_SEQ_LEN = 128\n",
        "\n",
        "print(\"Pre-processing train data...\")\n",
        "processed_docs_train = []\n",
        "for line in tqdm(raw_docs_train):\n",
        "    tokens = tokenizer.tokenize(line)\n",
        "    processed_docs_train.append(\" \".join(tokens))\n",
        "\n",
        "print(\"Pre-processing test data...\")\n",
        "processed_docs_test = []\n",
        "for line in tqdm(raw_docs_test):\n",
        "    tokens = tokenizer.tokenize(line)\n",
        "    processed_docs_test.append(\" \".join(tokens))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Qdp0-GUPLnl"
      },
      "outputs": [],
      "source": [
        "# Encode the processed docs\n",
        "word_seq_train = tokenizer.batch_encode_plus(\n",
        "    processed_docs_train,\n",
        "    padding=True,\n",
        "    truncation=True,\n",
        "    max_length=MAX_SEQ_LEN,\n",
        "    return_tensors=\"tf\"\n",
        ")\n",
        "word_seq_test = tokenizer.batch_encode_plus(\n",
        "    processed_docs_test,\n",
        "    padding=True,\n",
        "    truncation=True,\n",
        "    max_length=MAX_SEQ_LEN,\n",
        "    return_tensors=\"tf\"\n",
        ")\n",
        "\n",
        "word_seq_train = np.array(word_seq_train[\"input_ids\"])\n",
        "attention_mask_train = np.array(word_seq_train[\"attention_mask\"])\n",
        "word_seq_test = np.array(word_seq_test[\"input_ids\"])\n",
        "attention_mask_test = np.array(word_seq_test[\"attention_mask\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6qU-UfAK1GUZ"
      },
      "outputs": [],
      "source": [
        "pip install sentencepiece\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bTDR_aawNxXS"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModel, AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained('ai4bharat/indic-bert')\n",
        "model = AutoModel.from_pretrained('ai4bharat/indic-bert')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wt-EdWG0QAaR"
      },
      "outputs": [],
      "source": [
        "# Retrieve embeddings from the Indic-BERT model\n",
        "train_embeddings = model(word_seq_train, attention_mask=attention_mask_train).last_hidden_state\n",
        "test_embeddings = model(word_seq_test, attention_mask=attention_mask_test).last_hidden_state\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "43v1j1x0QDTg"
      },
      "outputs": [],
      "source": [
        "# Model parameters\n",
        "num_filters = 12\n",
        "weight_decay = 1e-4\n",
        "num_classes = 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Cb-2ll3QHqP"
      },
      "outputs": [],
      "source": [
        "\n",
        "# CNN architecture\n",
        "print(\"Training CNN...\")\n",
        "model = Sequential()\n",
        "model.add(Conv1D(num_filters, 7, activation='relu', padding='same', input_shape=(MAX_SEQ_LEN, train_embeddings.shape[2])))\n",
        "model.add(MaxPooling1D(2))\n",
        "model.add(Conv1D(num_filters, 7, activation='relu', padding='same'))\n",
        "model.add(GlobalMaxPooling1D())\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(32, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(weight_decay)))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "adam = tf.optimizers.Adam()\n",
        "model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ctagXZIMQKqR"
      },
      "outputs": [],
      "source": [
        "# Training params\n",
        "batch_size = 256\n",
        "num_epochs = 20\n",
        "checkpoint_path = \"/content/drive/MyDrive/Twitter_dataset/trainined_cnn_cp.ckpt\"\n",
        "checkpoint_dir = os.path.dirname(checkpoint_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ymhVTXuQO2Z"
      },
      "outputs": [],
      "source": [
        "# Create a callback that saves the model's weights\n",
        "cp_callback = ModelCheckpoint(filepath=checkpoint_path, save_weights_only=True, verbose=1)\n",
        "\n",
        "# Model training\n",
        "hist = model.fit(train_embeddings, y_train, batch_size=batch_size, epochs=num_epochs,\n",
        "                 validation_split=0.1, shuffle=True, verbose=1, callbacks=[cp_callback])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred = model_GRU.predict(word_seq_test)\n",
        "y_pred = np.argmax(y_pred, axis=1)\n",
        "\n",
        "# Convert one-hot encoded y_test to categorical labels\n",
        "y_test_labels = np.argmax(y_test, axis=1)\n",
        "\n",
        "# Calculate precision\n",
        "precision = precision_score(y_test_labels, y_pred)\n",
        "\n",
        "# Calculate recall\n",
        "recall = recall_score(y_test_labels, y_pred)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test_labels, y_pred)\n",
        "\n",
        "# Calculate F1 score\n",
        "f1 = f1_score(y_test_labels, y_pred)\n",
        "\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"F1 Score:\", f1)\n"
      ],
      "metadata": {
        "id": "edtvZwHC2iFS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "indicbert-cnn-language"
      ],
      "metadata": {
        "id": "H3I695lJ1yr1"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "flclXxP9yvRt"
      },
      "source": [
        "==================================================LSTM============================"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SchFvDJPZJpi"
      },
      "outputs": [],
      "source": [
        "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D, Bidirectional, Dropout"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "LV_Ghy317d0S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GXZ0zOrtY9ad"
      },
      "outputs": [],
      "source": [
        "#LSTM\n",
        "lstm_out = 128\n",
        "num_classes = 2\n",
        "model_LSTM = Sequential()\n",
        "model_LSTM.add(Embedding(nb_words, embed_dim,\n",
        "          weights=[embedding_matrix], input_length=max_seq_len, trainable=False))\n",
        "model_LSTM.add(Bidirectional(LSTM(lstm_out, dropout=0.2)))\n",
        "model_LSTM.add(Dense(128, activation = 'relu'))\n",
        "model_LSTM.add(Dropout(0.5))\n",
        "model_LSTM.add(Dense(64, activation = 'relu'))\n",
        "model_LSTM.add(Dense(num_classes, activation='softmax'))  #multi-label (k-hot encoding)\n",
        "adam = tf.optimizers.Adam()\n",
        "model_LSTM.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
        "model_LSTM.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qSjm45z_Y9af"
      },
      "outputs": [],
      "source": [
        "#training params\n",
        "batch_size_LSTM = 256\n",
        "num_epochs_LSTM = 20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y-tkl0lDY9af"
      },
      "outputs": [],
      "source": [
        "checkpoint_path = \"training/LSTM/language_detection/trained_cp.ckpt\"\n",
        "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "\n",
        "# Create a callback that saves the model's weights\n",
        "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
        "                                                 save_weights_only=True,\n",
        "                                                 verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pXlp-BcMY9ag"
      },
      "outputs": [],
      "source": [
        "#model training_LSTM\n",
        "hist_LSTM = model_LSTM.fit(word_seq_train, y_train, batch_size=batch_size_LSTM, epochs=num_epochs_LSTM,validation_split=0.1, shuffle=True, verbose=1,callbacks=[cp_callback])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred = model_LSTM.predict(word_seq_test)\n",
        "y_pred = np.argmax(y_pred, axis=1)\n",
        "\n",
        "# Convert one-hot encoded y_test to categorical labels\n",
        "y_test_labels = np.argmax(y_test, axis=1)\n",
        "\n",
        "# Calculate precision\n",
        "precision = precision_score(y_test_labels, y_pred)\n",
        "\n",
        "# Calculate recall\n",
        "recall = recall_score(y_test_labels, y_pred)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test_labels, y_pred)\n",
        "\n",
        "# Calculate F1 score\n",
        "f1 = f1_score(y_test_labels, y_pred)\n",
        "\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"F1 Score:\", f1)\n"
      ],
      "metadata": {
        "id": "-XIxnR2l78iX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E6KYcO2oy6gL"
      },
      "source": [
        "indicbert-lstm-LANGUAGE"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "=================================================================================GRU-================="
      ],
      "metadata": {
        "id": "U4t8DBehzmIU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aY3IAQIfI5Rv"
      },
      "outputs": [],
      "source": [
        "from keras.layers import Dense, Embedding,  GRU, SpatialDropout1D, Bidirectional, Dropout"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vABe7GV_Inac"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import Embedding, Bidirectional, GRU, Dense, Dropout\n",
        "gru_out = 128\n",
        "num_classes = 2\n",
        "model_GRU = keras.Sequential()\n",
        "model_GRU.add(Embedding(nb_words, embed_dim, weights=[embedding_matrix], input_length=max_seq_len, trainable=False))\n",
        "model_GRU.add(Bidirectional(GRU(gru_out, dropout=0.2)))\n",
        "model_GRU.add(Dense(128, activation='relu'))\n",
        "model_GRU.add(Dropout(0.5))\n",
        "model_GRU.add(Dense(64, activation='relu'))\n",
        "model_GRU.add(Dense(num_classes, activation='softmax'))  # multi-label (k-hot encoding)\n",
        "adam = tf.optimizers.Adam()\n",
        "model_GRU.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
        "model_GRU.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T6dWsGkMRTeg"
      },
      "outputs": [],
      "source": [
        "#training params\n",
        "batch_size = 256\n",
        "num_epochs = 20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2MnRG9XlRX21"
      },
      "outputs": [],
      "source": [
        "checkpoint_path = \"/content/drive/MyDrive/Twitter_dataset/trainined_gru_cp.ckpt\"\n",
        "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "\n",
        "# Create a callback that saves the model's weights\n",
        "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
        "                                                  save_weights_only=True,\n",
        "                                                 verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JsWfe7KERdhN"
      },
      "outputs": [],
      "source": [
        "#model training\n",
        "hist = model_GRU.fit(word_seq_train, y_train, batch_size=batch_size, epochs=num_epochs,validation_split=0.1, shuffle=True, verbose=1,callbacks=[cp_callback])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred = model_GRU.predict(word_seq_test)\n",
        "y_pred = np.argmax(y_pred, axis=1)\n",
        "\n",
        "# Convert one-hot encoded y_test to categorical labels\n",
        "y_test_labels = np.argmax(y_test, axis=1)\n",
        "\n",
        "# Calculate precision\n",
        "precision = precision_score(y_test_labels, y_pred)\n",
        "\n",
        "# Calculate recall\n",
        "recall = recall_score(y_test_labels, y_pred)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test_labels, y_pred)\n",
        "\n",
        "# Calculate F1 score\n",
        "f1 = f1_score(y_test_labels, y_pred)\n",
        "\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"F1 Score:\", f1)\n"
      ],
      "metadata": {
        "id": "S2crVJib7Uk_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AUN05BpODcR5"
      },
      "source": [
        "**indicbert*-gru-LANGUAGE"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}