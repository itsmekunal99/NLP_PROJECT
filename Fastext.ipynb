{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "PXUsJEKsirK_",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PXUsJEKsirK_",
        "outputId": "be640fee-5c0e-4cdb-bf51-7ba97f5ebc3b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "K91CFif4hYVu",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K91CFif4hYVu",
        "outputId": "5842dffc-20c9-4c15-ae68-2cedfad34894"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting fasttext\n",
            "  Downloading fasttext-0.9.2.tar.gz (68 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.8/68.8 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pybind11>=2.2 (from fasttext)\n",
            "  Using cached pybind11-2.10.4-py3-none-any.whl (222 kB)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from fasttext) (67.7.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fasttext) (1.22.4)\n",
            "Building wheels for collected packages: fasttext\n",
            "  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.2-cp310-cp310-linux_x86_64.whl size=4393319 sha256=ae96376efe53d6d625bb4123b373a5c59ac0199d1397f8b8e67cc34088e6117e\n",
            "  Stored in directory: /root/.cache/pip/wheels/a5/13/75/f811c84a8ab36eedbaef977a6a58a98990e8e0f1967f98f394\n",
            "Successfully built fasttext\n",
            "Installing collected packages: pybind11, fasttext\n",
            "Successfully installed fasttext-0.9.2 pybind11-2.10.4\n"
          ]
        }
      ],
      "source": [
        "!pip install fasttext"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "ac9aa959",
      "metadata": {
        "id": "ac9aa959"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "#import seaborn as sns\n",
        "#import matplotlib.pyplot as plt\n",
        "import keras\n",
        "from keras import optimizers\n",
        "from keras import backend as K\n",
        "from keras import regularizers\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Dropout, Flatten\n",
        "from keras.layers import Embedding, Conv1D, MaxPooling1D, GlobalMaxPooling1D\n",
        "from keras.preprocessing import sequence\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.callbacks import EarlyStopping\n",
        "from tqdm import tqdm\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "import os, re, csv, math, codecs\n",
        "import fasttext as ft\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "gLROTBQThgqI",
      "metadata": {
        "id": "gLROTBQThgqI"
      },
      "outputs": [],
      "source": [
        "# Construct a tf.data.Dataset\n",
        "data = pd.read_csv('/content/drive/MyDrive/processed_sentiment_140_en_mr.csv',encoding='utf-8', names = ['polarity','id','text','lang'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "qAzKDJiPrIkW",
      "metadata": {
        "id": "qAzKDJiPrIkW"
      },
      "outputs": [],
      "source": [
        "data= data.dropna().reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "DO8KwHbxfk1H",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "DO8KwHbxfk1H",
        "outputId": "948e18a6-5667-4b0a-e23f-ccc868ee3cd3"
      },
      "outputs": [],
      "source": [
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "id": "oGEEtWKdh1cM",
      "metadata": {
        "id": "oGEEtWKdh1cM"
      },
      "outputs": [],
      "source": [
        "#processing label of training/testing data\n",
        "label_data=data['polarity'].values\n",
        "senti=[0,4]\n",
        "mapping = {}\n",
        "for x in range(len(senti)):\n",
        "    mapping[senti[x]] = x\n",
        "\n",
        "# integer representation\n",
        "for x in range(len(label_data)):\n",
        "    label_data[x] = mapping[label_data[x]]\n",
        "\n",
        "#conveting to one-hot-encoding\n",
        "y_data = to_categorical(label_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "nNRi_CTNi9xO",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nNRi_CTNi9xO",
        "outputId": "97ac4e64-5660-4176-cda7-1bc65d94f0a4"
      },
      "outputs": [],
      "source": [
        "y_data[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "id": "418ed000",
      "metadata": {
        "id": "418ed000"
      },
      "outputs": [],
      "source": [
        "# Text cleaning function\n",
        "def text_cleaner(line):\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                               u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
        "                               u\"\\U00002702-\\U000027B0\"\n",
        "                               u\"\\U00002702-\\U000027B0\"\n",
        "                               u\"\\U000024C2-\\U0001F251\"\n",
        "                               u\"\\U0001f926-\\U0001f937\"\n",
        "                               u\"\\U00010000-\\U0010ffff\"\n",
        "                               u\"\\u2640-\\u2642\"\n",
        "                               u\"\\u2600-\\u2B55\"\n",
        "                               u\"\\u200d\"\n",
        "                               u\"\\u23cf\"\n",
        "                               u\"\\u2066\"\n",
        "                               u\"\\u2069\"\n",
        "                               u\"\\u23e9\"\n",
        "                               u\"\\u231a\"\n",
        "                               u\"\\ufe0f\"  # dingbats\n",
        "                               u\"\\u3030\"\n",
        "                               \"]+\", flags=re.UNICODE)\n",
        "    #remove punctuation and special characters\n",
        "    #line= re.sub(r'[^\\w\\s]','',line)\n",
        "    punc = '''؛₹|!‼→¿()-[]{};–৷।:،۔•„'“‘—’”\"…\\,<>=./?@#$%^&*_~»«'''\n",
        "    line=re.sub(r'http\\S+', '', line)\n",
        "    line=emoji_pattern.sub(r'', line)\n",
        "    line = bytes(line, 'utf-8').decode('utf-8', 'ignore')\n",
        "    for ele in line:\n",
        "        if ele in punc:\n",
        "            line = line.replace(ele, \"\")\n",
        "            line=re.sub(r\"^\\s+|\\s+$\", \"\", line)\n",
        "            line = re.sub(' +', ' ', line) # removing extra white spaceS\n",
        "            #line= clean(line, no_emoji=True)\n",
        "    return line"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "id": "6b4ec69a",
      "metadata": {
        "id": "6b4ec69a"
      },
      "outputs": [],
      "source": [
        "# import WhitespaceTokenizer() method from nltk\n",
        "from nltk.tokenize import WhitespaceTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "id": "xJyJOhpOjsvW",
      "metadata": {
        "id": "xJyJOhpOjsvW"
      },
      "outputs": [],
      "source": [
        "data_x=data['text'].tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "id": "ErOs2yZNjqP1",
      "metadata": {
        "id": "ErOs2yZNjqP1"
      },
      "outputs": [],
      "source": [
        "raw_docs_train, raw_docs_test, y_train, y_test = train_test_split(data_x, y_data,  test_size=0.2, random_state=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08d27f88",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "08d27f88",
        "outputId": "e451f129-df76-41de-d957-98c6cb4deb3b"
      },
      "outputs": [],
      "source": [
        "#pre_processing train/test data\n",
        "MAX_NB_WORDS = 100000\n",
        "# tokenizer = RegexpTokenizer(r'\\w+')\n",
        "tokenizer = WhitespaceTokenizer()\n",
        "\n",
        "# raw_docs_train = train_df[1].tolist()\n",
        "# raw_docs_test = test_df[1].tolist()\n",
        "num_classes = 2\n",
        "\n",
        "print(\"pre-processing train data...\")\n",
        "processed_docs_train = []\n",
        "for line in tqdm(raw_docs_train):\n",
        "    line=text_cleaner(line)\n",
        "    tokens = tokenizer.tokenize(line)\n",
        "    #filtered = [word for word in tokens if word not in stop_words]\n",
        "    processed_docs_train.append(\" \".join(tokens))\n",
        "#end for\n",
        "print(\"pre-processing test data...\")\n",
        "processed_docs_test = []\n",
        "for line in tqdm(raw_docs_test):\n",
        "    #line=text_cleaner(line)\n",
        "    tokens = tokenizer.tokenize(line)\n",
        "    #filtered = [word for word in tokens if word not in stop_words]\n",
        "    processed_docs_test.append(\" \".join(tokens))\n",
        "#end for\n",
        "\n",
        "data['doc_len'] = data['text'].apply(lambda words: len(words.split(\" \")))\n",
        "max_seq_len = np.round(data['doc_len'].mean() + data['doc_len'].std()).astype(int)\n",
        "\n",
        "print(\"tokenizing input data...\")\n",
        "tokenizer = Tokenizer(num_words=MAX_NB_WORDS, lower=True, char_level=False)\n",
        "tokenizer.fit_on_texts(processed_docs_train + processed_docs_test)  #leaky\n",
        "word_seq_train = tokenizer.texts_to_sequences(processed_docs_train)\n",
        "word_seq_test = tokenizer.texts_to_sequences(processed_docs_test)\n",
        "word_index = tokenizer.word_index\n",
        "print(\"dictionary size: \", len(word_index))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "id": "oXo7SCI2lpkt",
      "metadata": {
        "id": "oXo7SCI2lpkt"
      },
      "outputs": [],
      "source": [
        "#pad sequences\n",
        "word_seq_train = keras.utils.pad_sequences(word_seq_train, maxlen=max_seq_len)\n",
        "word_seq_test = keras.utils.pad_sequences(word_seq_test, maxlen=max_seq_len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "90c86756",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "90c86756",
        "outputId": "97b439d3-3df3-4fb4-a313-dd9d64996f03"
      },
      "outputs": [],
      "source": [
        "#loading embedding\n",
        "import fasttext as ft\n",
        "ft_model = ft.load_model(\"/content/drive/MyDrive/cc.mr.300.bin\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9203c49d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9203c49d",
        "outputId": "7a4172ec-e908-4951-8c2f-5649566396cd"
      },
      "outputs": [],
      "source": [
        "#embedding matrix\n",
        "embed_dim = 300\n",
        "print('preparing embedding matrix...')\n",
        "words_not_found = []\n",
        "nb_words = min(MAX_NB_WORDS, len(word_index)+1)\n",
        "embedding_matrix = np.zeros((nb_words, embed_dim))\n",
        "for word, i in word_index.items():\n",
        "    if i >= nb_words:\n",
        "        continue\n",
        "    embedding_vector = ft_model.get_word_vector(word)\n",
        "    if (embedding_vector is not None) and len(embedding_vector) > 0:\n",
        "        # words not found in embedding index will be all-zeros.\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "    else:\n",
        "        words_not_found.append(word)\n",
        "print('number of null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "id": "92a93279",
      "metadata": {
        "id": "92a93279"
      },
      "outputs": [],
      "source": [
        "#model parameters\n",
        "num_filters = 12\n",
        "weight_decay = 1e-4"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "mWOIO190gkuP",
      "metadata": {
        "id": "mWOIO190gkuP"
      },
      "source": [
        "==============================cnn======================"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b384a08f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b384a08f",
        "outputId": "0b0e9b47-a87d-4b01-f843-a76ade225e00"
      },
      "outputs": [],
      "source": [
        "# CNN architecture\n",
        "print(\"training CNN ...\")\n",
        "model = Sequential()\n",
        "model.add(Embedding(nb_words, embed_dim,\n",
        "          weights=[embedding_matrix], input_length=max_seq_len, trainable=False))\n",
        "model.add(Conv1D(num_filters,  7, activation='relu', padding='same'))\n",
        "model.add(MaxPooling1D(2))\n",
        "model.add(Conv1D(num_filters, 7, activation='relu', padding='same'))\n",
        "model.add(GlobalMaxPooling1D())\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(32, activation='relu', kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model.add(Dense(num_classes, activation='softmax'))  #multi-label (k-hot encoding)\n",
        "adam = tf.optimizers.Adam()\n",
        "model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "id": "a1b9ab57",
      "metadata": {
        "id": "a1b9ab57"
      },
      "outputs": [],
      "source": [
        "#training params\n",
        "batch_size = 256\n",
        "num_epochs = 20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "id": "8b53bfa0",
      "metadata": {
        "id": "8b53bfa0"
      },
      "outputs": [],
      "source": [
        "checkpoint_path = \"/content/drive/MyDrive/Twitter_dataset/trainined_cnn_cp.ckpt\"\n",
        "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "\n",
        "# Create a callback that saves the model's weights\n",
        "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
        "                                                  save_weights_only=True,\n",
        "                                                 verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a900b01",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2a900b01",
        "outputId": "e22559a1-bd43-4264-e371-394a4f87a9c7"
      },
      "outputs": [],
      "source": [
        "#model training\n",
        "hist = model.fit(word_seq_train, y_train, batch_size=batch_size, epochs=num_epochs,validation_split=0.1, shuffle=True, verbose=1,callbacks=[cp_callback])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2XQoQ6o0DNI7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2XQoQ6o0DNI7",
        "outputId": "1cb0f4f9-f644-41eb-e3be-dc069226e7c9"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred = model.predict(word_seq_test)\n",
        "y_pred = np.argmax(y_pred, axis=1)\n",
        "\n",
        "# Convert one-hot encoded y_test to categorical labels\n",
        "y_test_labels = np.argmax(y_test, axis=1)\n",
        "\n",
        "# Calculate precision\n",
        "precision = precision_score(y_test_labels, y_pred)\n",
        "\n",
        "# Calculate recall\n",
        "recall = recall_score(y_test_labels, y_pred)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test_labels, y_pred)\n",
        "\n",
        "# Calculate F1 score\n",
        "f1 = f1_score(y_test_labels, y_pred)\n",
        "\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"F1 Score:\", f1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "_hlSfQ1eRMiQ",
      "metadata": {
        "id": "_hlSfQ1eRMiQ"
      },
      "source": [
        "fastext-cnn-LANGUAGE"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "SotfPyxqWymL",
      "metadata": {
        "id": "SotfPyxqWymL"
      },
      "source": [
        "==============================================LSTM===================================================="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "id": "1d20245f",
      "metadata": {
        "id": "1d20245f"
      },
      "outputs": [],
      "source": [
        "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D, Bidirectional, Dropout"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "47b2f2b6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "47b2f2b6",
        "outputId": "f7cb9d00-9e87-4fde-8cfc-d7a65bb7f244"
      },
      "outputs": [],
      "source": [
        "#LSTM\n",
        "lstm_out = 128\n",
        "model_LSTM = keras.Sequential()\n",
        "model_LSTM.add(Embedding(nb_words, embed_dim,\n",
        "          weights=[embedding_matrix], input_length=max_seq_len, trainable=False))\n",
        "model_LSTM.add(Bidirectional(LSTM(lstm_out, dropout=0.2)))\n",
        "model_LSTM.add(Dense(128, activation = 'relu'))\n",
        "model_LSTM.add(Dropout(0.5))\n",
        "model_LSTM.add(Dense(64, activation = 'relu'))\n",
        "model_LSTM.add(Dense(num_classes, activation='softmax'))  #multi-label (k-hot encoding)\n",
        "adam = tf.optimizers.Adam()\n",
        "model_LSTM.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
        "model_LSTM.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "id": "28d21ac0",
      "metadata": {
        "id": "28d21ac0"
      },
      "outputs": [],
      "source": [
        "#training params\n",
        "batch_size_LSTM = 256\n",
        "num_epochs_LSTM = 20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "id": "560da513",
      "metadata": {
        "id": "560da513"
      },
      "outputs": [],
      "source": [
        "checkpoint_path = \"training/LSTM/language_detection/trained_cp.ckpt\"\n",
        "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "\n",
        "# Create a callback that saves the model's weights\n",
        "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
        "                                                 save_weights_only=True,\n",
        "                                                 verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d35059ea",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d35059ea",
        "outputId": "738bbe4f-84fb-43e0-ce70-e6d3e4a81cf8"
      },
      "outputs": [],
      "source": [
        "#model training_LSTM\n",
        "hist_LSTM = model_LSTM.fit(word_seq_train, y_train, batch_size=batch_size_LSTM, epochs=num_epochs_LSTM,validation_split=0.1, shuffle=True, verbose=1,callbacks=[cp_callback])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "OmLp2mRXhSpz",
      "metadata": {
        "id": "OmLp2mRXhSpz"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred = model_LSTM.predict(word_seq_test)\n",
        "y_pred = np.argmax(y_pred, axis=1)\n",
        "\n",
        "# Convert one-hot encoded y_test to categorical labels\n",
        "y_test_labels = np.argmax(y_test, axis=1)\n",
        "\n",
        "# Calculate precision\n",
        "precision = precision_score(y_test_labels, y_pred)\n",
        "\n",
        "# Calculate recall\n",
        "recall = recall_score(y_test_labels, y_pred)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test_labels, y_pred)\n",
        "\n",
        "# Calculate F1 score\n",
        "f1 = f1_score(y_test_labels, y_pred)\n",
        "\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"F1 Score:\", f1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "QEVb4MpBhY2_",
      "metadata": {
        "id": "QEVb4MpBhY2_"
      },
      "source": [
        "fastext-lstm-LANGUAGE"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "xnTnivaLg3WL",
      "metadata": {
        "id": "xnTnivaLg3WL"
      },
      "source": [
        "====================================\n",
        " GRU  =========================="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aY3IAQIfI5Rv",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "aY3IAQIfI5Rv"
      },
      "outputs": [],
      "source": [
        "from keras.layers import Dense, Embedding,  GRU, SpatialDropout1D, Bidirectional, Dropout"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vABe7GV_Inac",
      "metadata": {
        "id": "vABe7GV_Inac"
      },
      "outputs": [],
      "source": [
        "gru_out = 128\n",
        "model_GRU = keras.Sequential()\n",
        "model_GRU.add(Embedding(nb_words, embed_dim, weights=[embedding_matrix], input_length=max_seq_len, trainable=False))\n",
        "model_GRU.add(Bidirectional(GRU(gru_out, dropout=0.2)))\n",
        "model_GRU.add(Dense(128, activation='relu'))\n",
        "model_GRU.add(Dropout(0.5))\n",
        "model_GRU.add(Dense(64, activation='relu'))\n",
        "model_GRU.add(Dense(num_classes, activation='softmax'))  # multi-label (k-hot encoding)\n",
        "adam = tf.optimizers.Adam()\n",
        "model_GRU.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
        "model_GRU.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "T6dWsGkMRTeg",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "T6dWsGkMRTeg"
      },
      "outputs": [],
      "source": [
        "#training params\n",
        "batch_size = 256\n",
        "num_epochs = 20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2MnRG9XlRX21",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "2MnRG9XlRX21"
      },
      "outputs": [],
      "source": [
        "checkpoint_path = \"/content/drive/MyDrive/Twitter_dataset/trainined_gru_cp.ckpt\"\n",
        "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "\n",
        "# Create a callback that saves the model's weights\n",
        "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
        "                                                  save_weights_only=True,\n",
        "                                                 verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "JsWfe7KERdhN",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "JsWfe7KERdhN"
      },
      "outputs": [],
      "source": [
        "#model training\n",
        "hist = model_GRU.fit(word_seq_train, y_train, batch_size=batch_size, epochs=num_epochs,validation_split=0.1, shuffle=True, verbose=1,callbacks=[cp_callback])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7PT_rMrNhupR",
      "metadata": {
        "id": "7PT_rMrNhupR"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred = model_GRU.predict(word_seq_test)\n",
        "y_pred = np.argmax(y_pred, axis=1)\n",
        "\n",
        "# Convert one-hot encoded y_test to categorical labels\n",
        "y_test_labels = np.argmax(y_test, axis=1)\n",
        "\n",
        "# Calculate precision\n",
        "precision = precision_score(y_test_labels, y_pred)\n",
        "\n",
        "# Calculate recall\n",
        "recall = recall_score(y_test_labels, y_pred)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test_labels, y_pred)\n",
        "\n",
        "# Calculate F1 score\n",
        "f1 = f1_score(y_test_labels, y_pred)\n",
        "\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"F1 Score:\", f1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc89e902",
      "metadata": {},
      "source": [
        "Muril-gru-LANGUAGE"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
